{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "NEBkh0JeL4Z3",
        "outputId": "c4d2c950-c68e-4e77-91b6-ab396ae15daa",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-82270417f675>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeypointrcnn_resnet50_fpn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_StorageBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypedStorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_LegacyStorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUntypedStorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_warn_typed_storage_removal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m from torch._namedtensor_internals import (\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mcheck_serializing_named_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mis_ellipsis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from google.colab import files\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.detection import keypointrcnn_resnet50_fpn\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore')\n",
        "\n",
        "# Load the pre-trained pose estimation model\n",
        "model = keypointrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Define the keypoints list (including fingers)\n",
        "keypoints_names = ['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',\n",
        "                   'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',\n",
        "                   'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
        "                   'left_knee', 'right_knee', 'left_ankle', 'right_ankle']\n",
        "\n",
        "# Create a dictionary to map keypoint names to their indices\n",
        "keypoints_dict = {keypoint: i for i, keypoint in enumerate(keypoints_names)}\n",
        "\n",
        "# Define the skeleton connections\n",
        "skeleton_connections = [\n",
        "    ('nose', 'left_eye'), ('nose', 'right_eye'),\n",
        "    ('left_eye', 'left_ear'), ('right_eye', 'right_ear'),\n",
        "    ('left_shoulder', 'right_shoulder'), ('left_shoulder', 'left_elbow'),\n",
        "    ('right_shoulder', 'right_elbow'), ('left_elbow', 'left_wrist'),\n",
        "    ('right_elbow', 'right_wrist'), ('left_shoulder', 'left_hip'),\n",
        "    ('right_shoulder', 'right_hip'), ('left_hip', 'left_knee'),\n",
        "    ('right_hip', 'right_knee'), ('left_knee', 'left_ankle'),\n",
        "    ('right_knee', 'right_ankle'),('left_hip','right_hip')\n",
        "]\n",
        "\n",
        "# Define the function to draw lines connecting keypoints to form a skeleton\n",
        "# Define the function to draw lines connecting keypoints to form a skeleton\n",
        "def draw_skeleton_lines_with_keypoints(img, keypoints, skeleton_connections):\n",
        "    # Draw skeleton lines\n",
        "    for connection in skeleton_connections:\n",
        "        start_keypoint, end_keypoint = connection\n",
        "        start_point = (int(keypoints[keypoints_dict[start_keypoint]][0]), int(keypoints[keypoints_dict[start_keypoint]][1]))\n",
        "        end_point = (int(keypoints[keypoints_dict[end_keypoint]][0]), int(keypoints[keypoints_dict[end_keypoint]][1]))\n",
        "        cv2.line(img, start_point, end_point, (0, 255, 0), 2)\n",
        "\n",
        "    head_point = (int(keypoints[keypoints_dict['nose']][0]), int(keypoints[keypoints_dict['nose']][1]))\n",
        "    torso_point = (int((keypoints[keypoints_dict['left_shoulder']][0] + keypoints[keypoints_dict['right_shoulder']][0]) / 2),\n",
        "                   int((keypoints[keypoints_dict['left_shoulder']][1] + keypoints[keypoints_dict['right_shoulder']][1]) / 2))\n",
        "    torso_point_hip=(int((keypoints[keypoints_dict['left_hip']][0] + keypoints[keypoints_dict['right_hip']][0]) / 2),\n",
        "                   int((keypoints[keypoints_dict['left_hip']][1] + keypoints[keypoints_dict['right_hip']][1]) / 2))\n",
        "\n",
        "    cv2.line(img, head_point, torso_point, (0, 255, 0), 2)\n",
        "    cv2.line(img,torso_point,torso_point_hip,(0,255,0),2)\n",
        "\n",
        "\n",
        "\n",
        "    # Draw keypoints\n",
        "    for keypoint_name, keypoint_coords in keypoints_dict.items():\n",
        "        keypoint = (int(keypoints[keypoint_coords][0]), int(keypoints[keypoint_coords][1]))\n",
        "        cv2.circle(img, keypoint, 5, (255, 0, 0), -1)\n",
        "\n",
        "    cv2.circle(img, torso_point, 5, (255,0, 0), -1)  # Red circle for torso_point\n",
        "    cv2.circle(img, torso_point_hip, 5, (255, 0, 0), -1)  # Red circle for torso_point_hip\n",
        "\n",
        "\n",
        "# ... (rest of the code remains the same)\n",
        "uploaded = files.upload()\n",
        "video_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Initialize video capture\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Initialize video writer for output\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "out = cv2.VideoWriter('output_video_skeleton.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30, (frame_width, frame_height))\n",
        "\n",
        "# Define the transformation to apply to each frame\n",
        "transform = T.Compose([T.ToTensor()])\n",
        "# Process each frame of the video\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Preprocess the frame and make prediction\n",
        "    img_tensor = transform(frame)\n",
        "    with torch.no_grad():\n",
        "        prediction = model([img_tensor])\n",
        "\n",
        "    # Get keypoints from the prediction (customize this based on your model's output format)\n",
        "    keypoints = prediction[0]['keypoints'][0].numpy()\n",
        "\n",
        "    # Draw lines and keypoints to form a skeleton\n",
        "    draw_skeleton_lines_with_keypoints(frame, keypoints, skeleton_connections)\n",
        "\n",
        "    # Display the frame with skeleton lines and keypoints\n",
        "    _, img_encoded = cv2.imencode('.png', cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "    display(img_pil)\n",
        "\n",
        "    # Write the frame with skeleton lines and keypoints to the output video\n",
        "    out.write(frame)\n",
        "\n",
        "# ... (rest of the code remains the same)\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from google.colab import files\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.detection import keypointrcnn_resnet50_fpn\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore')\n",
        "\n",
        "# Load the pre-trained pose estimation model\n",
        "model = keypointrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Define the keypoints list (including fingers)\n",
        "keypoints_names = ['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',\n",
        "                   'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',\n",
        "                   'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
        "                   'left_knee', 'right_knee', 'left_ankle', 'right_ankle']\n",
        "\n",
        "# Create a dictionary to map keypoint names to their indices\n",
        "keypoints_dict = {keypoint: i for i, keypoint in enumerate(keypoints_names)}\n",
        "\n",
        "# Define the skeleton connections\n",
        "skeleton_connections = [\n",
        "    ('nose', 'left_eye'), ('nose', 'right_eye'),\n",
        "    ('left_eye', 'left_ear'), ('right_eye', 'right_ear'),\n",
        "    ('left_shoulder', 'right_shoulder'), ('left_shoulder', 'left_elbow'),\n",
        "    ('right_shoulder', 'right_elbow'), ('left_elbow', 'left_wrist'),\n",
        "    ('right_elbow', 'right_wrist'), ('left_shoulder', 'left_hip'),\n",
        "    ('right_shoulder', 'right_hip'), ('left_hip', 'left_knee'),\n",
        "    ('right_hip', 'right_knee'), ('left_knee', 'left_ankle'),\n",
        "    ('right_knee', 'right_ankle'), ('left_hip', 'right_hip')\n",
        "]\n",
        "\n",
        "# Define the function to draw lines connecting keypoints to form a skeleton\n",
        "def draw_skeleton_lines_with_keypoints(img, keypoints, skeleton_connections):\n",
        "    # Draw skeleton lines\n",
        "    for connection in skeleton_connections:\n",
        "        start_keypoint, end_keypoint = connection\n",
        "        start_point = (int(keypoints[keypoints_dict[start_keypoint]][0]), int(keypoints[keypoints_dict[start_keypoint]][1]))\n",
        "        end_point = (int(keypoints[keypoints_dict[end_keypoint]][0]), int(keypoints[keypoints_dict[end_keypoint]][1]))\n",
        "        cv2.line(img, start_point, end_point, (0, 255, 0), 2)\n",
        "\n",
        "    head_point = (int(keypoints[keypoints_dict['nose']][0]), int(keypoints[keypoints_dict['nose']][1]))\n",
        "    torso_point = (int((keypoints[keypoints_dict['left_shoulder']][0] + keypoints[keypoints_dict['right_shoulder']][0]) / 2),\n",
        "                   int((keypoints[keypoints_dict['left_shoulder']][1] + keypoints[keypoints_dict['right_shoulder']][1]) / 2))\n",
        "    torso_point_hip = (int((keypoints[keypoints_dict['left_hip']][0] + keypoints[keypoints_dict['right_hip']][0]) / 2),\n",
        "                      int((keypoints[keypoints_dict['left_hip']][1] + keypoints[keypoints_dict['right_hip']][1]) / 2))\n",
        "\n",
        "    cv2.line(img, head_point, torso_point, (0, 255, 0), 2)\n",
        "    cv2.line(img, torso_point, torso_point_hip, (0, 255, 0), 2)\n",
        "\n",
        "    # Draw keypoints\n",
        "    for keypoint_name, keypoint_coords in keypoints_dict.items():\n",
        "        keypoint = (int(keypoints[keypoint_coords][0]), int(keypoints[keypoint_coords][1]))\n",
        "        cv2.circle(img, keypoint, 5, (255, 0, 0), -1)\n",
        "\n",
        "    cv2.circle(img, torso_point, 5, (255, 0, 0), -1)  # Red circle for torso_point\n",
        "    cv2.circle(img, torso_point_hip, 5, (255, 0, 0), -1)  # Red circle for torso_point_hip\n",
        "\n",
        "#calculating angle\n",
        "def calculate_angle(point1, point2, point3):\n",
        "    vector1 = np.array([point1[0] - point2[0], point1[1] - point2[1]])\n",
        "    vector2 = np.array([point3[0] - point2[0], point3[1] - point2[1]])\n",
        "\n",
        "    dot_product = np.dot(vector1, vector2)\n",
        "    magnitude1 = np.linalg.norm(vector1)\n",
        "    magnitude2 = np.linalg.norm(vector2)\n",
        "\n",
        "    cosine_angle = dot_product / (magnitude1 * magnitude2)\n",
        "\n",
        "    angle = np.degrees(np.arccos(cosine_angle))\n",
        "    return angle\n",
        "\n",
        "\n",
        "# Define the function to estimate the action\n",
        "def estimate_action(keypoints):\n",
        "    # Extract relevant keypoints for action estimation\n",
        "    left_hip = keypoints[keypoints_dict['left_hip']]\n",
        "    right_hip = keypoints[keypoints_dict['right_hip']]\n",
        "    left_knee = keypoints[keypoints_dict['left_knee']]\n",
        "    right_knee = keypoints[keypoints_dict['right_knee']]\n",
        "    left_ankle = keypoints[keypoints_dict['left_ankle']]\n",
        "    right_ankle = keypoints[keypoints_dict['right_ankle']]\n",
        "    left_shoulder = keypoints[keypoints_dict['left_shoulder']]\n",
        "    right_shoulder = keypoints[keypoints_dict['right_shoulder']]\n",
        "    left_elbow = keypoints[keypoints_dict['left_elbow']]\n",
        "    right_elbow = keypoints[keypoints_dict['right_elbow']]\n",
        "    left_wrist = keypoints[keypoints_dict['left_wrist']]\n",
        "    right_wrist = keypoints[keypoints_dict['right_wrist']]\n",
        "\n",
        "    # Calculate the angles between hip, knee, and ankle keypoints\n",
        "    left_leg_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
        "    right_leg_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
        "\n",
        "    # Calculate the angles between shoulder, elbow, and wrist keypoints\n",
        "    left_arm_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
        "    right_arm_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n",
        "\n",
        "\n",
        "    #calculate the angles for wrist and elbow\n",
        "    left_horizontal_alignment = abs(left_wrist[1] - left_elbow[1]) < 10\n",
        "    right_horizontal_alignment = abs(right_wrist[1] - right_elbow[1]) < 10\n",
        "\n",
        "    # Thresholds for action classification (you may need to adjust these based on your observations)\n",
        "    standing_threshold = 150\n",
        "    walking_threshold = 120\n",
        "    waving_threshold = 120\n",
        "\n",
        "    # Classify the action based on the angles\n",
        "    if left_leg_angle > standing_threshold and right_leg_angle > standing_threshold:\n",
        "        return \"Standing\"\n",
        "    elif left_leg_angle < walking_threshold and right_leg_angle < walking_threshold:\n",
        "        return \"Running\"\n",
        "    elif left_arm_angle > waving_threshold or right_arm_angle > waving_threshold:\n",
        "        return \"Waving\"\n",
        "    elif left_horizontal_alignment and right_horizontal_alignment:\n",
        "        return \"Exercising\"\n",
        "    else:\n",
        "        return \"Walking\"\n",
        "uploaded = files.upload()\n",
        "video_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Initialize video capture\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Initialize video writer for output\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "out = cv2.VideoWriter('output_video_skeleton.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30, (frame_width, frame_height))\n",
        "\n",
        "# Define the transformation to apply to each frame\n",
        "transform = T.Compose([T.ToTensor()])\n",
        "\n",
        "# Inside the loop where you process each frame, call the estimate_action function\n",
        "# after obtaining the keypoints and display the estimated action\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Preprocess the frame and make prediction\n",
        "    img_tensor = transform(frame)\n",
        "    with torch.no_grad():\n",
        "        prediction = model([img_tensor])\n",
        "\n",
        "    # Get keypoints from the prediction (customize this based on your model's output format)\n",
        "    keypoints = prediction[0]['keypoints'][0].numpy()\n",
        "\n",
        "    # Draw lines and keypoints to form a skeleton\n",
        "    draw_skeleton_lines_with_keypoints(frame, keypoints, skeleton_connections)\n",
        "\n",
        "    # Estimate action\n",
        "    estimated_action = estimate_action(keypoints)\n",
        "\n",
        "    # Display the frame with skeleton lines, keypoints, and estimated action\n",
        "    cv2.putText(frame, f\"Action: {estimated_action}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "    _, img_encoded = cv2.imencode('.png', cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "    display(img_pil)\n",
        "\n",
        "    # Write the frame with skeleton lines, keypoints, and estimated action to the output video\n",
        "    out.write(frame)\n",
        "\n",
        "# ... (rest of the code remains the same)\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "56FocyvMbWvL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}